### Background

[Not In My Country](https://www.notinmycountry.org/) (NIMC) is a website that allows students in Uganda and Kenya to report instances of corruption from university staff. [DataKind DC](http://www.datakind.org/howitworks/datachapters/datakind-dc/) (DKDC) volunteers are teaming up to expand NIMC's scope to other countries and public officials in different sectors (e.g., judges).

### Repository

On DKDC's DataJam on March 16, 2015, volunteers were split into 2 groups.

* Searchers identified online open data about public officials in target countries. A [Hackpad document](https://hackpad.com/Not-In-My-Country-httpswww.notinmycountry.org-RrVYW0iowGI) lists the sources identified and categorizes them by country and best method to mine them (manual entry vs. scraping).

* Scrapers wrote initial code to scrape and/or format the data found by searchers. Their code, including appropriate documentation, is stored in the [scrapers](https://github.com/jm-contreras/not-in-my-country/tree/master/scrapers) directory.

### Introduction to GitHub

GitHub allows programmers to store and share code. It uses [git](http://en.wikipedia.org/wiki/Git_%28software%29) for version control, documenting how code evolves over time as you make additions, editions, and deletions. Additionally, GitHub makes it easy for multiple programmers to edit the same code, tracking who does what and when.

If you are new to GitHub, then please refer to their tutorials on [setting up](https://help.github.com/articles/set-up-git/) and [contributing to a repository](https://help.github.com/articles/making-changes/). More comprehensive introductions can be found online, like [GitHub For Beginners: Don't Get Scared, Get Started](http://readwrite.com/2013/09/30/understanding-github-a-journey-for-beginners-part-1).

### Introduction to Python Scraping

[Web scraping](http://en.wikipedia.org/wiki/Web_scraping) is the act of using a computer program to extract information from a website. Much of the data that we will want to collect for this project will require web scraping.
